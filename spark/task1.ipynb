{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix all jupyter notebook problems\n",
    "\n",
    "<sub> 0. Pray </sub> \n",
    "1. Connect to head machine via SSH\n",
    "2. Open `/usr/bin/anaconda/lib/python2.7/site-packages/nbformat/_version.py` and change 5 to 4.\n",
    "3. Fix anaconda installation via official fix script. \n",
    "```\n",
    "curl https://gregorysfixes.blob.core.windows.net/public/fix-conda.sh | sudo sh\n",
    "```\n",
    "4. Install all necessary python packages. At least kaggle - \n",
    "```\n",
    "sudo /usr/bin/anaconda/bin/conda install -c conda-forge kaggle --yes\n",
    "```\n",
    "5. Open Ambari and restart jupyter service.\n",
    "6. Open azure jupyter notebook and upload this notebook\n",
    "7. Check, that cells below can be executed correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 47.552978515625,
      "end_time": 1582237561341.93
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1582190762543_0008</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark2.nqjiyvt1lw2u5puwbzzkpomikc.bx.internal.cloudapp.net:8088/proxy/application_1582190762543_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-spark2.nqjiyvt1lw2u5puwbzzkpomikc.bx.internal.cloudapp.net:30060/node/containerlogs/container_1582190762543_0008_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 241.994873046875,
      "end_time": 1582237561595.04
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 246.215087890625,
      "end_time": 1582237561859.174
     }
    }
   },
   "outputs": [],
   "source": [
    "hadoop = sc._jvm.org.apache.hadoop\n",
    "fs = hadoop.fs.FileSystem\n",
    "conf = hadoop.conf.Configuration() \n",
    "path = hadoop.fs.Path('/')\n",
    "    \n",
    "def hdfs_ls(path):\n",
    "    result = []\n",
    "    for f in fs.get(conf).listStatus(hadoop.fs.Path(path)):\n",
    "        result.append(str(f.getPath()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 758.401123046875,
      "end_time": 1582237603500.463
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/custom-scriptaction-logs',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/app-logs',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/tmp',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/warehouse',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/mapred',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/HDInsight_TestAccessiblityBlobName',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/hive',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/example',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/apps',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/HdiSamples',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/yarn',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/HdiNotebooks',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/amshbase',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/ams',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/atshistory',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/hbase',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/hdp',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/user',\n",
      " 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/mr-history']"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(hdfs_ls('/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download task data\n",
    "\n",
    "Download data directly from kaggle. Read this to understand how: https://github.com/Kaggle/kaggle-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 210.26806640625,
      "end_time": 1582219662750.281
     }
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "! cat ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1054.541015625,
      "end_time": 1582220036186.809
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/spark/.kaggle/kaggle.json'\n",
      "name                           size  creationDate         \n",
      "----------------------------  -----  -------------------  \n",
      "clicks_test.csv.zip           135MB  2018-06-22 05:33:10  \n",
      "documents_entities.csv.zip    126MB  2018-06-22 05:33:10  \n",
      "documents_topics.csv.zip      121MB  2018-06-22 05:33:10  \n",
      "documents_categories.csv.zip   32MB  2018-06-22 05:33:10  \n",
      "page_views_sample.csv.zip     149MB  2018-06-22 05:33:10  \n",
      "clicks_train.csv.zip          390MB  2018-06-22 05:33:10  \n",
      "page_views.csv.zip             35GB  2018-06-22 05:33:10  \n",
      "events.csv.zip                478MB  2018-06-22 05:33:10  \n",
      "sample_submission.csv.zip     100MB  2018-06-22 05:33:10  \n",
      "promoted_content.csv.zip        3MB  2018-06-22 05:33:10  \n",
      "documents_meta.csv.zip         16MB  2018-06-22 05:33:10  \n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! kaggle competitions files outbrain-click-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1029.8291015625,
      "end_time": 1582237621450.165
     }
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "! kaggle competitions download outbrain-click-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/outbrain-click-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 4507.58203125,
      "end_time": 1582237971017.23
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/02/20 22:32:48 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n",
      "20/02/20 22:32:48 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Delete operation is: 37 ms with threads: 0\n",
      "Deleted /task1\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! hdfs dfs -rm -r /task1\n",
    "! hdfs dfs -mkdir /task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 5347.8388671875,
      "end_time": 1582237943122.283
     }
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "! for i in `ls *.zip`; do unzip -p $i | tqdm | hadoop fs -put - /task1/${i//\\.zip/}; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2246.673095703125,
      "end_time": 1582237727913.001
     }
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "! hadoop fs -du -s -h /task1/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pvdf = ss.read.csv(\"/task1/page_views.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n",
      "|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 ms, sys: 20 ms, total: 96 ms\n",
      "Wall time: 9min 16s\n"
     ]
    },
    {
     "data": {},
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pvdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet is faster than CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://events.linuxfoundation.org/sites/events/files/slides/ApacheCon%20BigData%20Europe%202016%20-%20Parquet%20in%20Practice%20%26%20Detail_0.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pvdf.write.parquet(\"/task1/page_views.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.3 G  /task1/page_views.parquet\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! hadoop fs -du -s -h /task1/page_views.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvdf2 = ss.read.parquet(\"/task1/page_views.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 22.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from IPython.display import display\n",
    "boo = pvdf2.groupBy(\"geo_location\").count().collect()\n",
    "display(boo[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 20 ms, total: 116 ms\n",
      "Wall time: 10min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "boo = pvdf.groupBy(\"geo_location\").count().collect()\n",
    "display(boo[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test.parquet done\n",
      "clicks_train.parquet done\n",
      "documents_categories.parquet done\n",
      "documents_entities.parquet done\n",
      "documents_meta.parquet done\n",
      "documents_topics.parquet done\n",
      "events.parquet done\n",
      "page_views.parquet done\n",
      "page_views_sample.parquet done\n",
      "promoted_content.parquet done\n",
      "sample_submission.parquet done\n",
      "CPU times: user 96 ms, sys: 0 ns, total: 96 ms\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def convert_all_to_parquet():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_ls(task_dir)\n",
    "    for fn in all_files:\n",
    "        if fn.endswith(\".csv\"):\n",
    "            fn_after = fn.replace(\".csv\", \".parquet\")\n",
    "            path_before = fn\n",
    "            path_after = fn_after\n",
    "            if fn_after not in all_files:\n",
    "                # generate parquet\n",
    "                df = ss.read.csv(path_before, header=True)\n",
    "                df.write.parquet(path_after)\n",
    "            print(fn_after, \"done\")\n",
    "\n",
    "convert_all_to_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove csv, we have parquet now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "! hdfs dfs -rm /task1/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133.2 M  /task1/clicks_test.parquet\n",
      "367.5 M  /task1/clicks_train.parquet\n",
      "36.5 M  /task1/documents_categories.parquet\n",
      "184.0 M  /task1/documents_entities.parquet\n",
      "21.2 M  /task1/documents_meta.parquet\n",
      "183.3 M  /task1/documents_topics.parquet\n",
      "669.3 M  /task1/events.parquet\n",
      "47.3 G  /task1/page_views.parquet\n",
      "236.9 M  /task1/page_views_sample.parquet\n",
      "5.0 M  /task1/promoted_content.parquet\n",
      "184.2 M  /task1/sample_submission.parquet\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! hadoop fs -du -s -h /task1/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### /task1/clicks_test.parquet ###############\n",
      "+----------+------+\n",
      "|display_id| ad_id|\n",
      "+----------+------+\n",
      "|  17805143|288388|\n",
      "+----------+------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/clicks_train.parquet ###############\n",
      "+----------+-----+-------+\n",
      "|display_id|ad_id|clicked|\n",
      "+----------+-----+-------+\n",
      "|         1|42337|      0|\n",
      "+----------+-----+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_categories.parquet ###############\n",
      "+-----------+-----------+----------------+\n",
      "|document_id|category_id|confidence_level|\n",
      "+-----------+-----------+----------------+\n",
      "|    1544588|       1513|     0.263546236|\n",
      "+-----------+-----------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_entities.parquet ###############\n",
      "+-----------+--------------------+-----------------+\n",
      "|document_id|           entity_id| confidence_level|\n",
      "+-----------+--------------------+-----------------+\n",
      "|    1539011|e01ed0c4a3e8f8f35...|0.327269624728567|\n",
      "+-----------+--------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_meta.parquet ###############\n",
      "+-----------+---------+------------+-------------------+\n",
      "|document_id|source_id|publisher_id|       publish_time|\n",
      "+-----------+---------+------------+-------------------+\n",
      "|     325048|      822|         253|2013-02-27 00:00:00|\n",
      "+-----------+---------+------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/documents_topics.parquet ###############\n",
      "+-----------+--------+------------------+\n",
      "|document_id|topic_id|  confidence_level|\n",
      "+-----------+--------+------------------+\n",
      "|     801028|     280|0.0148711250868194|\n",
      "+-----------+--------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/events.parquet ###############\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/page_views.parquet ###############\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|68fb8eb72c49c4|    1201414| 63621328|       3|       GB>F8|             2|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/page_views_sample.parquet ###############\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|7504d9623fdc7e|        234| 72194818|       1|   US>CA>825|             1|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/promoted_content.parquet ###############\n",
      "+-----+-----------+-----------+-------------+\n",
      "|ad_id|document_id|campaign_id|advertiser_id|\n",
      "+-----+-----------+-----------+-------------+\n",
      "|    1|       6614|          1|            7|\n",
      "+-----+-----------+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "############### /task1/sample_submission.parquet ###############\n",
      "+----------+--------------------+\n",
      "|display_id|               ad_id|\n",
      "+----------+--------------------+\n",
      "|  21960532|50582 190398 2293...|\n",
      "+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 28 ms, sys: 4 ms, total: 32 ms\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preview_all_files():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_ls(task_dir)\n",
    "    for fn in all_files:\n",
    "        df = ss.read.parquet(fn)\n",
    "        print(\"#\" * 15 + \" {0} \".format(fn) + \"#\" * 15)\n",
    "        df.show(1)\n",
    "        \n",
    "preview_all_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register all tables to be usable in SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_test done\n",
      "clicks_train done\n",
      "documents_categories done\n",
      "documents_categories2 done\n",
      "documents_entities done\n",
      "documents_meta done\n",
      "documents_topics done\n",
      "events done\n",
      "events_test done\n",
      "events_train done\n",
      "joined_events done\n",
      "page_views done\n",
      "page_views_sample done\n",
      "promoted_content done\n",
      "sample_submission done\n",
      "CPU times: user 36 ms, sys: 8 ms, total: 44 ms\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def register_all_tables():\n",
    "    task_dir = \"/task1/\"\n",
    "    all_files = hdfs_ls(task_dir)\n",
    "    for fn in all_files:\n",
    "        if fn.endswith(\".parquet\"):\n",
    "            table_name = fn.replace(\".parquet\", \"\")\n",
    "            df = ss.read.parquet(fn)\n",
    "            df.registerTempTable(table_name)\n",
    "            print(table_name, \"done\")\n",
    "        \n",
    "register_all_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 16.1 s\n"
     ]
    },
    {
     "data": {},
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ss.sql(\"\"\"\n",
    "select count(distinct(uuid)) as users_count\n",
    "from events\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple model using the following features:\n",
    "- **clicked**\n",
    "- geo_location features (country, state, dma)\n",
    "- day_of_week (from timestamp, use *date.isoweekday()*)\n",
    "- ad_id\n",
    "- ad_document_id\n",
    "- campaign_id\n",
    "- advertiser_id\n",
    "- display_document_id\n",
    "- platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate features for VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use DataFrame API to join tables (functions in SQL queries: https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html)\n",
    "- Use Python API to calculate features and save them as text for VW (*saveAsTextFile()*)\n",
    "- Hash features in Spark (24 bits, use *sklearn.utils.murmurhash.murmurhash3_32*)\n",
    "- Split dataset in Spark into 90% train, 10% test **by display_id**, save the split for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 5286.1611328125,
      "end_time": 1582238139803.682
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.murmurhash import murmurhash3_32\n",
    "def hasher(x, bits):\n",
    "    return murmurhash3_32(x) % 2**bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy data from HDFS to cluster machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run vowpal wabbit **locally**, need to copy data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "! hdfs dfs -getmerge /task1/train.txt train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "! hdfs dfs -getmerge /task1/test.txt test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to head node via SSH and install vw:\n",
    "\n",
    "```\n",
    "git clone --recursive https://github.com/VowpalWabbit/vowpal_wabbit.git\n",
    "sudo apt install libboost-dev libboost-program-options-dev libboost-system-dev libboost-thread-dev libboost-math-dev libboost-test-dev zlib1g-dev cmake g++ -y\n",
    "cd vowpal_wabbit && make && cd build && make install\n",
    "echo \"export PATH=/usr/local/bin:\\$PATH\" >> ~/.bashrc\n",
    "```\n",
    "\n",
    "Or you can download prebuilded vw \n",
    "```\n",
    "mkdir -p bin/\n",
    "wget http://finance.yendor.com/ML/VW/Binaries/vw-8.20190624 -O bin/vw\n",
    "chmod +x bin/vw\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1194.341796875,
      "end_time": 1582237013877.734
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-20 22:16:52--  http://finance.yendor.com/ML/VW/Binaries/vw-8.20190624\n",
      "Resolving finance.yendor.com (finance.yendor.com)... 69.163.152.190\n",
      "Connecting to finance.yendor.com (finance.yendor.com)|69.163.152.190|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9467376 (9.0M)\n",
      "Saving to: ‘vw’\n",
      "\n",
      "vw                  100%[===================>]   9.03M  13.7MB/s    in 0.7s    \n",
      "\n",
      "2020-02-20 22:16:53 (13.7 MB/s) - ‘vw’ saved [9467376/9467376]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! wget http://finance.yendor.com/ML/VW/Binaries/vw-8.20190624 -O vw\n",
    "! chmod +x vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 210.831787109375,
      "end_time": 1582237053042.234
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = \r\n",
      "num sources = 1\r\n",
      "driver:\r\n",
      "  --onethread           Disable parse thread\r\n",
      "VW options:\r\n",
      "  --ring_size arg (=256, ) size of example ring\r\n",
      "  --strict_parse           throw on malformed examples\r\n",
      "Update options:\r\n",
      "  -l [ --learning_rate ] arg Set learning rate\r\n",
      "  --power_t arg              t power value\r\n",
      "  --decay_learning_rate arg  Set Decay factor for learning_rate between passes\r\n",
      "  --initial_t arg            initial t value\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! ./vw --help | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 204.3740234375,
      "end_time": 1582237145215.502
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 |f 1748258:1.0 1376507:1.0 11602085:1.0 14125547:1.0 15617356:1.0 16621393:1.0 4204498:1.0 9609462:1.0 5728439:1.0 11418299:1.0\r\n",
      "-1 |f 5611969:1.0 11418299:1.0 11602085:1.0 15617356:1.0 15750989:1.0 786735:1.0 3083696:1.0 4204498:1.0 5728439:1.0 1376507:1.0\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! head -n2 train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 312.14306640625,
      "end_time": 1582237147864.006
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = model\r\n",
      "Enabling FTRL based optimization\r\n",
      "Algorithm used: Proximal-FTRL\r\n",
      "ftrl_alpha = 0.005\r\n",
      "ftrl_beta = 0.1\r\n",
      "Num weight bits = 24\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "creating cache_file = train.txt.cache\r\n",
      "Reading datafile = train.txt\r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples = 2\r\n",
      "weighted example sum = 2.000000\r\n",
      "weighted label sum = 0.000000\r\n",
      "average loss = 0.700492\r\n",
      "best constant = 0.000000\r\n",
      "best constant's loss = 0.693147\r\n",
      "total feature number = 22\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! LD_LIBRARY_PATH=/usr/local/lib ./vw -d train.txt -b 24 -c -k --ftrl --passes 1 -f model --holdout_off --loss_function logistic --random_seed 42 --progress 8000000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check VW test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 214.2021484375,
      "end_time": 1582237318373.902
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\r\n",
      "predictions = test_predictions.txt\r\n",
      "Enabling FTRL based optimization\r\n",
      "Algorithm used: Proximal-FTRL\r\n",
      "ftrl_alpha = 0.005\r\n",
      "ftrl_beta = 0.1\r\n",
      "Num weight bits = 24\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = test.txt\r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples = 2\r\n",
      "weighted example sum = 2.000000\r\n",
      "weighted label sum = 0.000000\r\n",
      "average loss = 0.966958\r\n",
      "best constant = 0.000000\r\n",
      "best constant's loss = 1.000000\r\n",
      "total feature number = 22\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! LD_LIBRARY_PATH=/usr/local/lib ./vw -d test.txt -i model -t -k -p test_predictions.txt --progress 1000000 --link=logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 207.689208984375,
      "end_time": 1582237359437.642
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.505993\r\n",
      "0.497650\r\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "! cat test_predictions.txt | head -n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 7.95703125,
      "end_time": 1582237326519.143
     }
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "import numpy as np\n",
    "\n",
    "def read_vw_predictions(p):\n",
    "    y_pred = []\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            y_pred.append(float(line.split()[0]))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "y_pred = read_vw_predictions(\"test_predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 5.323974609375,
      "end_time": 1582237326796.148
     }
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "def get_vw_y_true(p):\n",
    "    y_true = []\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            y_true.append(float(line.partition(\" \")[0]))\n",
    "    return np.array(y_true)\n",
    "\n",
    "y_true = get_vw_y_true(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2.138916015625,
      "end_time": 1582237328727.368
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "from sklearn.metrics import log_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 6.97900390625,
      "end_time": 1582237329208.456
     }
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "print(y_pred)\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 8.0791015625,
      "end_time": 1582237338761.081
     }
    }
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time let's make a personalized recommender using:\n",
    "- page views information\n",
    "- document properties\n",
    "\n",
    "Ideas for features:\n",
    "- uuid topic, entity, publisher, ... preferences\n",
    "- document similarities\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More SQL examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n",
      "|         2|79a85fa78311b9|    1794259|       81|       2|   US>CA>807|\n",
      "|         3|822932ce3d8757|    1179111|      182|       2|   US>MI>505|\n",
      "+----------+--------------+-----------+---------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we start with a DataFrame\n",
    "events_df = ss.sql(\"select * from events\")\n",
    "events_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can make RDD of Rows with *.rdd\n",
    "from pyspark.sql import Row\n",
    "events_df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When it's RDD, we can use Python to create new RDD of Rows\n",
    "(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n",
    ").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|           bar|          foo|\n",
      "+--------------+-------------+\n",
      "|cb8c55702adb93|[US, SC, 519]|\n",
      "|79a85fa78311b9|[US, CA, 807]|\n",
      "|822932ce3d8757|[US, MI, 505]|\n",
      "+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can convert it back to DataFrame if it's still a table that can be converted to Java types\n",
    "ss.createDataFrame(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 8 ms, total: 44 ms\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# we can save it to HDFS as parquet (if it's a DataFrame)\n",
    "ss.createDataFrame(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n",
    ").write.mode(\"overwrite\").parquet(\"/task1/example1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bar: string (nullable = true)\n",
      " |-- foo: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------+-------------+\n",
      "|           bar|          foo|\n",
      "+--------------+-------------+\n",
      "|cb8c55702adb93|[US, SC, 519]|\n",
      "|79a85fa78311b9|[US, CA, 807]|\n",
      "|822932ce3d8757|[US, MI, 505]|\n",
      "+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.read.parquet(\"/task1/example1\").printSchema()\n",
    "ss.read.parquet(\"/task1/example1\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# or we can skip DataFrame API if we use Python functions (there will be no speed increase)\n",
    "(\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n",
    ").saveAsPickleFile(\"/task1/example2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pickleFile(\"/task1/example2\").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sometimes we cannot make a DataFrame\n",
    "import numpy as np\n",
    "rdd = (\n",
    "    events_df.rdd\n",
    "    .map(lambda x: Row(x=np.array(x.geo_location.split(\">\") if x.geo_location else [])))\n",
    ")\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throws TypeError: not supported type: <type 'numpy.ndarray'>\n",
    "ss.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 8 ms, total: 36 ms\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# but we can save as RDD in pickle file just fine\n",
    "rdd.saveAsPickleFile(\"/task1/example3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways:\n",
    "- use DataFrames when you can (simple join's, select's, groupby's), it will be faster\n",
    "- use RDD and Python when you can't use DataFrame API\n",
    "- convert it back to DataFrame if needed\n",
    "- or save to pickles (can save almost any Python object as pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in SQL functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more at https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|document_id|          categories|\n",
      "+-----------+--------------------+\n",
      "|     100010|[[1513,0.79842798...|\n",
      "|    1000240|[[1505,0.92], [15...|\n",
      "|    1000280|[[1909,0.92], [19...|\n",
      "+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql version\n",
    "df = ss.sql(\"\"\"\n",
    "select\n",
    "    document_id,\n",
    "    collect_list(struct(category_id, confidence_level)) as categories\n",
    "from\n",
    "    documents_categories\n",
    "group by document_id\n",
    "\"\"\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.write.mode(\"overwrite\").parquet(\"/task1/example4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or we can use RDD and Python if we are not aware of those SQL functions\n",
    "rdd = (\n",
    "    ss.sql(\"select * from documents_categories\")\n",
    "    .rdd\n",
    "    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n",
    "    .groupByKey()\n",
    "    .map(lambda (k, vs): (k, list(vs)))\n",
    ")\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 12 ms, total: 40 ms\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# it's much slower, but we can do almost everything in Python\n",
    "rdd.saveAsPickleFile(\"/task1/example5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but sometimes with Python we can do more\n",
    "rdd = (\n",
    "    ss.sql(\"select * from documents_categories\")\n",
    "    .rdd\n",
    "    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n",
    "    .groupByKey()\n",
    "    .map(lambda (k, vs): Row(document_id=k, categories={a: float(b) for a, b in vs}))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 4 ms, total: 36 ms\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\n",
    "ss.createDataFrame(rdd).write.parquet(\"/task1/example6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|          categories|document_id|\n",
      "+--------------------+-----------+\n",
      "|Map(1510 -> 0.887...|    1059269|\n",
      "|Map(1408 -> 0.92,...|    1050604|\n",
      "|Map(1903 -> 0.92,...|    1472688|\n",
      "+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.read.parquet(\"/task1/example6\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we can join this table with events for instance\n",
    "ss.read.parquet(\"/task1/example6\").registerTempTable(\"doc_categories_ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------+---------+--------+------------+--------------------+\n",
      "|display_id|          uuid|document_id|timestamp|platform|geo_location|          categories|\n",
      "+----------+--------------+-----------+---------+--------+------------+--------------------+\n",
      "|  18242074|e703634e3dfa39|    1000240|536236046|       2|          NG|Map(1503 -> 0.07,...|\n",
      "|  18694427|5b023d28c0a9f3|    1000240|687121504|       2|   US>MA>521|Map(1503 -> 0.07,...|\n",
      "|   3436070|55e1db49ff4eef|    1000240|223783698|       1|   US>CA>803|Map(1503 -> 0.07,...|\n",
      "+----------+--------------+-----------+---------+--------+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"\"\"\n",
    "select \n",
    "    e.*, \n",
    "    dc.categories\n",
    "from \n",
    "    events as e\n",
    "    join doc_categories_ready as dc on dc.document_id = e.document_id\n",
    "\"\"\").show(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
